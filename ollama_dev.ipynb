{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "from pydantic import BaseModel\n",
    "import markdown_to_json\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model=\"agent_pm\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "\n",
    "llm_json = Ollama(\n",
    "    model=\"agent_json_fixer\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema:\n",
      "1. A markdown heading (#) with the title tasks, followed by a desciption of the project split into tasks as a markdown list.\n",
      "2. A markdown heading (#) with the title timeline, followed by a desciption of the time required for each task as a markdown list.\n",
      "3. A markdown heading (#) with the title deliverables, followed by a desciption of the project deliverables with scope as a markdown list.\n",
      "4. A markdown heading (#) with the title team, followed by a desciption of the required project team's skills and experience level as a markdown list.\n",
      "5. A markdown heading (#) with the title risks, followed by a desciption of the project risks as a markdown list.\n",
      "6. A markdown heading (#) with the title budget, followed by a desciption of the budget for each task as a markdown list.\n",
      "7. A markdown heading (#) with the title metrics, followed by a desciption of the project metrics as a markdown list.\n",
      "Do not return a response as a multilevel list when lists are used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ResponseComponent(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    output_format: str\n",
    "\n",
    "tasks = ResponseComponent(\n",
    "        name=\"tasks\",\n",
    "        description=\"the project split into tasks\",\n",
    "        output_format=\"markdown list\",\n",
    "    )\n",
    "timeline = ResponseComponent(\n",
    "        name=\"timeline\",\n",
    "        description=\"the time required for each task\",\n",
    "        output_format=\"markdown list\",\n",
    "    )\n",
    "deliverables =  ResponseComponent(\n",
    "        name=\"deliverables\",\n",
    "        description=\"the project deliverables with scope\",\n",
    "        output_format=\"markdown list\",\n",
    "    )\n",
    "team = ResponseComponent(\n",
    "        name=\"team\",\n",
    "        description=\"the required project team's skills and experience level\",\n",
    "        output_format=\"markdown list\",\n",
    "    )\n",
    "risks = ResponseComponent(\n",
    "        name=\"risks\",\n",
    "        description=\"the project risks\",\n",
    "        output_format=\"markdown list\",\n",
    "    )\n",
    "budget = ResponseComponent(\n",
    "        name=\"budget\",\n",
    "        description=\"the budget for each task\",\n",
    "        output_format=\"markdown list\",\n",
    "    )\n",
    "metrics = ResponseComponent(\n",
    "        name=\"metrics\",\n",
    "        description=\"the project metrics\",\n",
    "        output_format=\"markdown list\",\n",
    "    )\n",
    "\n",
    "def structured_output_instructions_from_response_components(response_components: list[ResponseComponent]):\n",
    "    base_instruction = \"\"\"\\\n",
    "The output should be a markdown code snippet formatted in the following schema:\n",
    "\"\"\"\n",
    "    for n, response_component in enumerate(response_components):\n",
    "        base_instruction += f\"{n+1}. A markdown heading (#) with the title {response_component.name}, followed by a desciption of {response_component.description} as a {response_component.output_format}.\\n\"\n",
    "    base_instruction += \"Do not return a response as a multilevel list when lists are used.\\n\"\n",
    "    \n",
    "    return base_instruction\n",
    "\n",
    "response_components = [\n",
    "    tasks,\n",
    "    timeline,\n",
    "    deliverables,\n",
    "    team,\n",
    "    risks,\n",
    "    budget,\n",
    "    metrics,\n",
    "]\n",
    "\n",
    "response_format = structured_output_instructions_from_response_components(response_components)\n",
    "\n",
    "print(response_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Tasks:\n",
      "\n",
      "1. Define the problem and identify the key factors affecting revenue forecasting.\n",
      "\t* Research and analyze the current forecasting process of the firm.\n",
      "\t* Identify the sources of external market data that can be used to improve forecasting accuracy.\n",
      "2. Collect and preprocess the external market data.\n",
      "\t* Determine the relevant market data sources for the firm's industries or competencies.\n",
      "\t* Clean and normalize the data to ensure consistency and accuracy.\n",
      "3. Develop a machine learning model to predict revenue.\n",
      "\t* Choose appropriate algorithms and techniques based on the problem requirements.\n",
      "\t* Train the model using the preprocessed market data and the firm's historical revenue data.\n",
      "4. Evaluate and refine the model.\n",
      "\t* Test the model with a holdout sample to evaluate its performance.\n",
      "\t* Identify areas for improvement and refine the model as needed.\n",
      "5. Implement the model in the firm's forecasting process.\n",
      "\t* Integrate the model into the existing forecasting system or develop a new one.\n",
      "\t* Train the model with the latest market data to ensure accuracy over time.\n",
      "6. Monitor and maintain the model.\n",
      "\t* Regularly evaluate the model's performance and update it as needed.\n",
      "\t* Ensure that the model remains accurate and relevant to the firm's changing needs.\n",
      "\n",
      "## Timeline:\n",
      "\n",
      "1. Define the problem and identify key factors affecting revenue forecasting: 2 weeks\n",
      "2. Collect and preprocess external market data: 4 weeks\n",
      "3. Develop a machine learning model to predict revenue: 8 weeks\n",
      "4. Evaluate and refine the model: 4 weeks\n",
      "5. Implement the model in the firm's forecasting process: 2 weeks\n",
      "6. Monitor and maintain the model: Ongoing (2-3 months)\n",
      "\n",
      "## Deliverables:\n",
      "\n",
      "1. A machine learning model that can accurately predict revenue for the firm.\n",
      "2. A detailed report on the current forecasting process and areas for improvement.\n",
      "3. A presentation of the project findings and recommendations to the firm's management.\n",
      "4. A user manual for the implemented model, including instructions on how to use it in the firm's forecasting process.\n",
      "\n",
      "## Team:\n",
      "\n",
      "1. Project manager with experience in managing complex projects and coordinating teams.\n",
      "2. Data scientist with expertise in machine learning and data analysis.\n",
      "3. Data analyst with experience in collecting, preprocessing, and evaluating market data.\n",
      "4. Business analyst with knowledge of the firm's industries or competencies and their revenue forecasting processes.\n",
      "5. Software developer with experience in integrating machine learning models into existing systems.\n",
      "6. Quality assurance specialist to ensure that the model is accurate and reliable.\n",
      "\n",
      "## Risks:\n",
      "\n",
      "1. Inaccurate or incomplete market data, which can affect the model's performance.\n",
      "2. Difficulty in integrating the model into the firm's existing forecasting process.\n",
      "3. Resistance to change from the firm's management and employees.\n",
      "4. Technical difficulties in developing and implementing the model.\n",
      "5. Insufficient accuracy of the model, which can result in suboptimal revenue forecasts.\n",
      "\n",
      "## Budget:\n",
      "\n",
      "1. Project manager: $X\n",
      "2. Data scientist: $Y\n",
      "3. Data analyst: $Z\n",
      "4. Business analyst: $W\n",
      "5. Software developer: $V\n",
      "6. Quality assurance specialist: $U\n",
      "Total budget: $X + $Y + $Z + $W + $V + $U\n",
      "\n",
      "## Metrics:\n",
      "\n",
      "1. Accuracy of the revenue forecasts compared to the actual revenue.\n",
      "2. Time savings in the forecasting process due to the use of the machine learning model.\n",
      "3. Increase in revenue forecasted by the model compared to the current process.\n",
      "4. Employee satisfaction with the new forecasting process."
     ]
    }
   ],
   "source": [
    "template = \"\"\"\\\n",
    "You have been given the following project brief. Identify and plan the key project tasks step by step.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Project Brief:\n",
    "{brief}\n",
    "\"\"\"\n",
    "\n",
    "brief = \"\"\"\\\n",
    "- The project is for a big multinational firm.\n",
    "- The firm employs about 10000 people.\n",
    "- The firm is grouped into a hierarchical structure, where each group specialises in a different industry or competency.\n",
    "- The firm is struggling to forecast its revenue accurately.\n",
    "- We are proposing to use machine learning an algorithms coupled with sourcing external market data to improve forecasting.\n",
    "- We aim to achieve a better accuracy than their current forecasting process.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"brief\"],\n",
    "    partial_variables={\"format_instructions\": response_format}\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(brief=brief)\n",
    "output_plan = llm(_input.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasks 18\n",
      "timeline 6\n",
      "deliverables 4\n",
      "team 6\n",
      "risks 5\n",
      "budget 6\n",
      "metrics 4\n"
     ]
    }
   ],
   "source": [
    "output_plan_dict = markdown_to_json.dictify(output_plan)\n",
    "\n",
    "\n",
    "def find_headings(response_components: ResponseComponent, text: str):\n",
    "    if type(text) != str:\n",
    "        return None\n",
    "    text_to_test = text.lower()\n",
    "    for component in response_components:\n",
    "        if component.name.lower() in text_to_test and len(text_to_test) <= len(component.name)*2:\n",
    "            return component.name\n",
    "\n",
    "result_dict = {}\n",
    "last_heading = None\n",
    "for l in output_plan_dict:\n",
    "    result = find_headings(response_components, l)\n",
    "    if result is not None:\n",
    "        result_dict[result] = output_plan_dict[l]\n",
    "\n",
    "def flatten_list(the_list: list, nr_prefix=''):\n",
    "    new_list = []\n",
    "    n = 0\n",
    "    for list_item in the_list:  \n",
    "        if type(list_item) == list:\n",
    "            number_str = f\"{nr_prefix}{str(n)}.\"\n",
    "            new_list.extend(flatten_list(list_item, nr_prefix=number_str))\n",
    "        else:\n",
    "            n += 1\n",
    "            number_str = f\"{nr_prefix}{str(n)}.\"\n",
    "            new_list.append(f\"{number_str} {str(list_item)}\")\n",
    "    return new_list\n",
    "\n",
    "for k, v in result_dict.items():\n",
    "    result_dict[k] = flatten_list(v)\n",
    "    # print(k, len(result_dict[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Project manager with experience in managing complex projects and coordinating teams.',\n",
       " '2. Data scientist with expertise in machine learning and data analysis.',\n",
       " '3. Data analyst with experience in collecting, preprocessing, and evaluating market data.',\n",
       " \"4. Business analyst with knowledge of the firm's industries or competencies and their revenue forecasting processes.\",\n",
       " '5. Software developer with experience in integrating machine learning models into existing systems.',\n",
       " '6. Quality assurance specialist to ensure that the model is accurate and reliable.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(json.dumps(output_plan_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tasks\n",
      "\n",
      "## Define the problem and identify key factors affecting revenue forecasting\n",
      "\n",
      "* Identify the current forecasting process of the firm and its accuracy\n",
      "* Determine the sources of external market data that can be used to improve forecasting accuracy\n",
      "* Identify the relevant industries or competencies of the firm for which market data should be collected\n",
      "\n",
      "## Research and analyze the current forecasting process of the firm\n",
      "\n",
      "* Evaluate the current forecasting methodology and tools used by the firm\n",
      "* Determine the strengths, weaknesses, opportunities, and threats (SWOT analysis) of the firm's forecasting process\n",
      "* Identify areas for improvement in the current forecasting process\n",
      "\n",
      "## Identify sources of external market data that can be used to improve forecasting accuracy\n",
      "\n",
      "* Determine the relevant market data sources for the firm's industries or competencies\n",
      "* Evaluate the quality and relevance of each market data source\n",
      "* Identify any potential biases or limitations in the market data sources\n",
      "\n",
      "## Collect and preprocess external market data\n",
      "\n",
      "* Determine the format and structure of the collected market data\n",
      "* Clean and normalize the data to ensure consistency and accuracy\n",
      "* Remove any outliers or irrelevant data points\n",
      "\n",
      "## Develop a machine learning model to predict revenue\n",
      "\n",
      "### Choose appropriate algorithms and techniques\n",
      "\n",
      "* Evaluate different machine learning algorithms and techniques based on the problem requirements\n",
      "* Select the most appropriate algorithm and technique for the task at hand\n",
      "\n",
      "### Train the model using preprocessed market data and firm's historical revenue data\n",
      "\n",
      "* Prepare the training data set using the collected market data and the firm's historical revenue data\n",
      "* Split the data set into a training, validation, and test set\n",
      "* Train the machine learning model using the training set\n",
      "* Evaluate the performance of the model using the validation set\n",
      "* Fine-tune the model as needed using the test set\n",
      "\n",
      "### Evaluate and refine the model\n",
      "\n",
      "* Test the model with a holdout sample to evaluate its performance\n",
      "* Identify areas for improvement and refine the model as needed\n",
      "* Continuously monitor and update the model over time to ensure accuracy\n",
      "\n",
      "## Implement the model in the firm's forecasting process\n",
      "\n",
      "### Integrate the model into the existing forecasting system or develop a new one\n",
      "\n",
      "* Determine the best way to integrate the model into the existing forecasting system or develop a new one\n",
      "* Ensure that the model is properly maintained and updated over time\n",
      "\n",
      "### Train the model with latest market data to ensure accuracy over time\n",
      "\n",
      "* Regularly update the model with the latest market data to ensure accuracy over time\n",
      "* Monitor the performance"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jurgenstrydom/Projects/llm_agent_team/ollama_dev.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jurgenstrydom/Projects/llm_agent_team/ollama_dev.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jurgenstrydom/Projects/llm_agent_team/ollama_dev.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     template\u001b[39m=\u001b[39mtemplate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jurgenstrydom/Projects/llm_agent_team/ollama_dev.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mbrief\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jurgenstrydom/Projects/llm_agent_team/ollama_dev.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     partial_variables\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mformat_instructions\u001b[39m\u001b[39m\"\u001b[39m: response_format}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jurgenstrydom/Projects/llm_agent_team/ollama_dev.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jurgenstrydom/Projects/llm_agent_team/ollama_dev.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m _input \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat_prompt(brief\u001b[39m=\u001b[39mbrief, tasks\u001b[39m=\u001b[39mresult_dict[\u001b[39m'\u001b[39m\u001b[39mtasks\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jurgenstrydom/Projects/llm_agent_team/ollama_dev.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m task_plan \u001b[39m=\u001b[39m llm(_input\u001b[39m.\u001b[39;49mto_string())\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/langchain/llms/base.py:876\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    870\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     )\n\u001b[1;32m    875\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    877\u001b[0m         [prompt],\n\u001b[1;32m    878\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    879\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    880\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m    881\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    882\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    883\u001b[0m     )\n\u001b[1;32m    884\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    886\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/langchain/llms/base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    646\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m     ]\n\u001b[0;32m--> 656\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    657\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/langchain/llms/base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    543\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 544\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    545\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/langchain/llms/base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    523\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    528\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    529\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 531\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    532\u001b[0m                 prompts,\n\u001b[1;32m    533\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    534\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    535\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    536\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    537\u001b[0m             )\n\u001b[1;32m    538\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    539\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    540\u001b[0m         )\n\u001b[1;32m    541\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/langchain/llms/ollama.py:220\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m generations \u001b[39m=\u001b[39m []\n\u001b[1;32m    219\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m--> 220\u001b[0m     final_chunk \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_stream_with_aggregation(\n\u001b[1;32m    221\u001b[0m         prompt,\n\u001b[1;32m    222\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    223\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m    224\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    225\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m     generations\u001b[39m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    228\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/langchain/llms/ollama.py:156\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    154\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GenerationChunk:\n\u001b[1;32m    155\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mfor\u001b[39;49;00m stream_resp \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_stream(prompt, stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs):\n\u001b[1;32m    157\u001b[0m         \u001b[39mif\u001b[39;49;00m stream_resp:\n\u001b[1;32m    158\u001b[0m             chunk \u001b[39m=\u001b[39;49m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[39m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[39m=\u001b[39;49mchunk_size, decode_unicode\u001b[39m=\u001b[39;49mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[39mif\u001b[39;49;00m pending \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[39m=\u001b[39;49m pending \u001b[39m+\u001b[39;49m chunk\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/requests/utils.py:571\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    570\u001b[0m decoder \u001b[39m=\u001b[39m codecs\u001b[39m.\u001b[39mgetincrementaldecoder(r\u001b[39m.\u001b[39mencoding)(errors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m iterator:\n\u001b[1;32m    572\u001b[0m     rv \u001b[39m=\u001b[39;49m decoder\u001b[39m.\u001b[39;49mdecode(chunk)\n\u001b[1;32m    573\u001b[0m     \u001b[39mif\u001b[39;49;00m rv:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/urllib3/response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[39mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[39m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[39m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 624\u001b[0m     \u001b[39mfor\u001b[39;49;00m line \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_chunked(amt, decode_content\u001b[39m=\u001b[39;49mdecode_content):\n\u001b[1;32m    625\u001b[0m         \u001b[39myield\u001b[39;49;00m line\n\u001b[1;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/urllib3/response.py:828\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_chunk_length()\n\u001b[1;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    830\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/llm_agent_team-CchbYHaz/lib/python3.11/site-packages/urllib3/response.py:758\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    757\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline()\n\u001b[1;32m    759\u001b[0m line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    760\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "template = \"\"\"\\\n",
    "Given the following project brief, and a list of tasks to solve for the brief. Take each task in the list and plan it step by step in detail.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Project Brief:\n",
    "{brief}\n",
    "\n",
    "Tasks:\n",
    "{tasks}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"brief\"],\n",
    "    partial_variables={\"format_instructions\": response_format}\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(brief=brief, tasks=result_dict['tasks'])\n",
    "task_plan = llm(_input.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agent_team-CchbYHaz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
